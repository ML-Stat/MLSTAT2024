---
title: "Tutorial"
summary: "2:00 p.m. — 6:00 p.m., Thursday, Aug 24, 2023"
tags: "t"
weight: 9
---

Thursday, Aug. 24, 2023
------


<hr style="border: 0; border-top: 5px solid;">

<!-- <div class="tip">
    <img class="icon" src="/icon/yanjiang.png" />
    SessionKenote Speech: <span class="font-bold" style="font-size:120%">TBA</span>
</div> -->

<div class="tip">
    <img class="icon" src="/icon/shizhong.png" />
    Time: 2:00 p.m. — 6:00 p.m.
</div>
<div class="tip">
    <img class="icon" src="/icon/didian.png" />
    location: room 101
</div>


<div class="tip">
    <img class="icon" src="/icon/lingdao.png" />
    Host: <a href="http://XXXXX" target="_blank">TBA</a>
</div>


________________________________________

<div class="row">
    <div class="left">
        <img src="/images/zhihua.png" class="avatar" />
        <div class="font-small font-bold">
            Zhihua Zhou
        </div>
    </div>
    <div class="right">
        <div class="font-small">
            <b>Title:</b>&nbsp;
            构建人工智能的基座模型：技术、挑战和未来
        </div>
        <div class="content font-small">
            <b>Abstract:</b> &nbsp;
            自从OpenAI发布了ChatGPT，大语言模型(LLM)引起了社会各界广发关注和遐想，同时也衍生了各种大模型的应用场景开发热潮。大语言模型的构建是一个复杂而又精细的巨系统，它不仅牵涉到数据质量、算力分配，而且同样取决于工程技艺、算法实现细节等。这个报告主要讨论构建大模型的一些技术问题，比如, 大模型基本组件，数据清洗，分词(Tokenization), 对齐(Alignment) 等。同时从Scaling Law和Compression角度来讨论理解大模型的机理。最后报告也试图分享个体或学术届在大模型研发的机会和作为，以及未来通用人工智能的潜在方向。
        </div>
    </div>
</div>

<div class="row">
    <div class="left">
        <img src="/images/yuling.png" class="avatar" />
        <div class="font-small font-bold">
            Yuling Jiao
        </div>
    </div>
    <div class="right">
        <div class="font-small">
            <b>Title:</b>&nbsp;
            Theoretical Study on Deep Learning: Approximation, Generalization, Optimization, Representation and Generation
        </div>
        <div class="content font-small">
            <b>Abstract:</b> &nbsp;
            In the first part of this talk, I will discuss some theoretical studies on deep learning with a focus on approximation, generalization, optimization, and representation. In particular, I will cover error analysis with over-parameterization. In the second part, I will delve into sampling and generative learning via  and SDE and ODE.
        </div>
    </div>
</div>


<style>

.tip {
    height: 30px;
    line-height: 30px;
}

.icon {
    width: 15px;
}

.row {
    padding: 10px; 
    height: auto; 
    border-bottom-width: 2px; 
    border-style: solid; 
    border-color: #E4E7ED; 
    padding-bottom: 20px; 
    padding-top: 20px;
    display: flex; 
    text-align: justify;
}

.left {
    min-width: 150px !important;
    text-align: center;
}

.avatar {
    width: 120px;
    height: 160px;
    max-width: 100%;
    border-radius: 10px;
}

.right {
    margin-left: 10px; 
    max-width: 80%;
}


.font-small {
    /* font-size: 16px; */
}

.font-bold {
    font-weight: bold;
}
</style>