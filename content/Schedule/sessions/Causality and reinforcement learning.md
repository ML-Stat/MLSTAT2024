---
title: "Causality and reinforcement learning"
summary: "1:30 p.m. — 3:00 p.m., Friday, Aug. 23, 2024"
tags: "s1-1"
weight: 12
---

Friday, Aug. 23, 2024
------


<hr style="border: 0; border-top: 5px solid;">

<div class="tip">
    <img class="icon" src="/images/yanjiang.png" />
    Session: <span class="font-bold" style="font-size:120%">Causality and reinforcement learning</span>
</div>

<div class="tip">
    <img class="icon" src="/images/shizhong.png" />
    Time: 1:30 p.m. — 3:00 p.m.
</div>
<div class="tip">
    <img class="icon" src="/images/didian.png" />
    Location: 东北师范大学 唯真楼xx
</div>


<div class="tip">
    <img class="icon" src="/images/lingdao.png" />
    Session Chair: 李伟，xxx University；常象宇，西安交通大学
</div>


________________________________________

<html>
<head>
    <title>Causal discovery from subsampled time series data</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            /* padding: 20px; */
            background-color: #f5f5f5;
        }
        .row {
            display: flex;
            justify-content: center;
        }
        .container {
            max-width: 800px;
            background-color: #ffffff;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            margin: 10px;
        }
        .name-bar {
            font-size: 24px;
            font-weight: bold;
            color: #333;
            margin-bottom: 10px;
        }
        .institute {
            font-size: 18px;
            color: #333;
            margin-bottom: 10px;
        }
        .title {
            font-size: 20px;
            text-decoration: underline;
            margin-bottom: 10px;
        }
        .abstract {
            font-size: 20px;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="row">
        <div class="container">
            <div class="name-bar">孙鑫伟</div>
            <div class="institute">Fu Dan University</div>
            <div class="title">Title: Causal discovery from subsampled time series data</div>
            <div class="abstract">
                <strong>Abstract:</strong> Inferring causal structures from time series data is the central interest of many areas. A major barrier to such inference is the problem of unobserveness due to subsampling, i.e., the frequency of measurement is lower than that of causal influence. To overcome this problem, numerous methods have been proposed, yet either was limited to the linear case or failed to achieve identifiability. In this talk, we will introduce a hypothesis testing procedure with proxy variables, which can identify the entire causal structure from subsampled time series, without any parametric constraint.We begin by framing the structure identification as a hypothesis testing problem, leveraging that each latent variable serves as an observed proxy at some future observable time point. We proceed to design a valid hypothesis testing procedure over continuous variables based on discretization,which under completeness conditions, is able to asymptotically establish a linear equation whose coefficient vector is identifiable under the causal null hypothesis. Asymptotic level and power, along with the results of False Discovery Rate control, are provided. Finally, we presentnumerical experimental results
            </div>
            <div class="abstract">
                <strong>Self introduction:</strong> 复旦大学
            </div>
        </div>
    </div>
    <div class="row">
        <div class="container">
            <div class="name-bar">张宇谦</div>
            <div class="institute">Xi'an Jiaotong University</div>
            <div class="title">Title: The power of depth in deep Q-Learning</div>
            <div class="abstract">
                <strong>Abstract:</strong> With the help of massive data and rich computational resource, deep Q-learning has been widely used in operations research and management science and receives great success in numerous applications including, recommender system, games and robotic manipulation. Compared with avid research activities in practice, there lack solid theoretical verifications and interpretability for the success of deep Q-learning, making it be a little bit mystery. The aim of this talk is to discuss the power of depth in deep Q-learning. In the framework of learning theory, we rigorously prove that deep Q-learning outperforms the traditional one by showing its good generalization error bound.  Our results show that the main reason of the success of deep Q-learning is due to the excellent performance of  deep neural networks (deep nets) in capturing special properties of rewards such as the spatially sparse and piecewise constant rather than due to their large capacities. In particular, we provide answers to questions why and when deep Q-learning performs better than the traditional one and how about the generalization capability of deep Q-learning.
            </div>
            <div class="abstract">
                <strong>Self Introduction:</strong> 西安交通大学管理学院，教授、博士生导师。研究方向为函数逼近论、分布式学习理论、深度学习理论及强化学习理论。在应用数学顶级期刊ACHA、SINUM、SISC及机器学习顶级期刊JMLR，TPAMI，TIT等发表论文80余篇。主持或以核心骨干参与国家级课题11项。
            </div>
        </div>
    </div>
    <div class="row">
        <div class="container">
            <div class="name-bar">林绍波</div>
            <div class="institute">Xi'an Jiaotong University</div>
            <div class="title">Title: The power of depth in deep Q-Learning</div>
            <div class="abstract">
                <strong>Abstract:</strong> With the help of massive data and rich computational resource, deep Q-learning has been widely used in operations research and management science and receives great success in numerous applications including, recommender system, games and robotic manipulation. Compared with avid research activities in practice, there lack solid theoretical verifications and interpretability for the success of deep Q-learning, making it be a little bit mystery. The aim of this talk is to discuss the power of depth in deep Q-learning. In the framework of learning theory, we rigorously prove that deep Q-learning outperforms the traditional one by showing its good generalization error bound.  Our results show that the main reason of the success of deep Q-learning is due to the excellent performance of  deep neural networks (deep nets) in capturing special properties of rewards such as the spatially sparse and piecewise constant rather than due to their large capacities. In particular, we provide answers to questions why and when deep Q-learning performs better than the traditional one and how about the generalization capability of deep Q-learning.
            </div>
            <div class="abstract">
                <strong>Self Introduction:</strong> 西安交通大学管理学院，教授、博士生导师。研究方向为函数逼近论、分布式学习理论、深度学习理论及强化学习理论。在应用数学顶级期刊ACHA、SINUM、SISC及机器学习顶级期刊JMLR，TPAMI，TIT等发表论文80余篇。主持或以核心骨干参与国家级课题11项。 
            </div>
        </div>
    </div>
    <div class="row">
        <div class="container">
            <div class="name-bar">张良宇</div>
            <div class="institute">Shanghai University of Finance and Economics</div>
            <div class="title">Title: Estimation and Inference in Distributional Reinforcement Learning </div>
            <div class="abstract">
                <strong>Abstract:</strong> Classical reinforcement learning relies on the 'reward hypothesis,' where the performance of a learning agent is assessed based on its expected returns. However, in many applications, it is not enough to merely consider the expected returns, because other factors such as uncertainty or risks can be crucial. To address this challenge, distributional reinforcement learning extends beyond the notion of expected returns, introducing the idea of learning the complete return distribution.In this talk,  we discuss the topics of estimation and inference in distributional reinforcement learning. Our investigation focuses on distributional policy evaluation, aiming to estimate the distribution of the return (denoted $\eta^\pi$) attained by a given policy $\pi$. We show that a polynomial number of samples can guarantee a near-optimal estimation when the estimator $\hat\eta^\pi$ is constructed by the certainty-equivalence method. We also examine the asymptotics of $\sqrt{\hat\eta^\pi-\eta^\pi}$ and show that it converges weakly to a Gaussian random element. Based on this, we propose a unified inference procedure for a wide class of statistical functionals of $\eta^\pi$.
            </div>
            <div class="abstract">
                <strong>Self Introduction:</strong> 上海财经大学统计与管理学院助理教授。2024年博士毕业于北京大学。主要研究方向包括分布鲁棒优化，强化学习理论。论文发表于Annals of Statistic，IEEE TPAMI等统计和机器学习期刊以及NeurIPS等机器学习会议。
            </div>
        </div>
    </div>
</body>
</html>

<style>

.tip{}

.icon {
    width: 15px;
}

.row {
    padding: 10px; 
    height: auto; 
    border-bottom-width: 2px; 
    border-style: solid; 
    border-color: #E4E7ED; 
    padding-bottom: 20px; 
    padding-top: 20px;
    display: flex; 
    text-align: justify;
}

.left {
    min-width: 150px !important;
    text-align: center;
}

.avatar {
    width: 120px;
    height: 160px;
    max-width: 100%;
    border-radius: 10px;
}

.right {
    margin-left: 10px; 
    max-width: 80%;
}


.font-small {
    /* font-size: 16px; */
}

.font-bold {
    font-weight: bold;
}
</style>