---
title: "Deep Learning in the Era of Large-scale Models"
summary: "1:30 p.m. — 3:00 p.m., Friday, Aug. 25, 2023"
tags: "s1-3"
weight: 50
---

Friday, Aug. 25, 2023
------


<hr style="border: 0; border-top: 5px solid;">

<div class="tip">
    <img class="icon" src="/icon/yanjiang.png" />
    Session: <span class="font-bold" style="font-size:120%">Deep Learning in the Era of Large-scale Models</span>
</div>

<div class="tip">
    <img class="icon" src="/icon/shizhong.png" />
    Time: 1:30 p.m. — 3:00 p.m.
</div>
<div class="tip">
    <img class="icon" src="/icon/didian.png" />
    location: 华东师范大学中北校区 文史楼211
</div>


<div class="tip">
    <img class="icon" src="/icon/lingdao.png" />
    Session Chair: Ling Zhou, Southwestern University of Finance and Economics
</div>


________________________________________

<html>
<head>
    <title>Customizing Personal Large-Scale Language Model</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .row {
            display: flex;
            justify-content: center;
        }
        .container {
            max-width: 800px;
            background-color: #ffffff;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            margin: 10px;
        }
        .name-bar {
            font-size: 24px;
            font-weight: bold;
            color: #333;
            margin-bottom: 10px;
        }
        .title {
            font-size: 20px;
            text-decoration: underline;
            margin-bottom: 10px;
        }
        .abstract {
            font-size: 20px;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="row">
        <div class="container">
            <div class="name-bar">Bin Liu</div>
            <div class="title">Title: Customizing Personal Large-Scale Language Model Using Co-Occurrence Statistic Information</div>
            <div class="abstract">
                <strong>Abstract:</strong> In text generation, a large language model (LLM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to match the topic of generated text with the current task. To fully explore such important information, we propose a graphsoftmax function for task-specific text generation. It is expected that the final word choice would be determined by both the global knowledge from the LLM and the local knowledge from the scene-specific corpus. To achieve this goal, we regularize the traditional softmax function with a graph total variation, which incorporates the local knowledge into the LLM. The proposed graphsoftmax can be plugged into a large pre-trained LLM for text generation and machine translation. Through experiments, we demonstrate that the new GTV-based regularization yields better performances in comparison with existing methods. Human testers can also easily distinguish the text generated by the graphsoftmax or softmax.
            </div>
        </div>
    </div>
    <div class="row">
        <div class="container">
            <div class="name-bar">Shaogao Lv</div>
            <div class="title">Title: Robust Structure Learning and L_p-Regularization for Graph Neural Networks</div>
            <div class="abstract">
                <strong>Abstract:</strong> Graph neural networks (GNNs) have become one of the most important branches in various deep learning, due to their remarkable power in learning with graph-structured data. Our current report consists of two folds.  First, we provide a lower bound of Rademacher complexity for two-layer GCNs, which motivates us to formulate the proposed robust algorithm for recovering graph structure and learning tasks in GCNs. Second, we also aims at quantifying the trade off of GCN between smoothness and sparsity, with the help of a new L_p-regularized (1 < p ≤ 2) stochastic learning proposed in the work. For a single-layer GCN, we develop an explicit theoretical understanding of GCN with the L_p-regularized stochastic learning by analyzing the stability of our regularized stochastic algorithm. Finally, several empirical experiments are implemented to validate our theoretical findings.
            </div>
        </div>
    </div>
    <div class="row">
        <div class="container">
            <div class="name-bar">Fengmao Lv</div>
            <div class="title">Title: 面向社交平台内容挖掘的多模态深度学习技术</div>
            <div class="abstract">
                <strong>Abstract:</strong> 随着知乎、微博、Twitter等社交平台的普遍流行，人们习惯于在社交平台进行分享。针对社交平台中的海量数据进行实时分析，有助于在舆情监控、犯罪检测、灾害防护、情感分析等方面有效提升相关职能部门的管理效率。由于多媒体技术和移动互联网技术的快速发展，社交平台近年来呈现出多模态的特性。除了传统的文字信息，视频、图片等内容在社交平台上普遍存在。为了提升对社交平台的分析质量，多模态深度学习近年来被广泛研究。通过整合语音、图像、语言等不同模态的信息，多模态深度学习能够显著提升对多模态互联网内容的识别能力。本报告将围绕多模态深度学习，介绍其相关技术原理和发展现状，并探讨大模型时代的多模态深度学习研究。
            </div>
        </div>
    </div>
    <div class="row">
        <div class="container">
            <div class="name-bar">Jingran Zhou</div>
            <div class="title">Title: Supervised Random Feature Regression via Projection Pursuit</div>
            <div class="abstract">
                <strong>Abstract:</strong> Random feature methods and neural network models are two popular nonparametric modeling methods, which are regarded as representatives of shallow learning and Neural Network, respectively. In practice random feature methods are short of the capacity of feature learning, while neural network methods lead to computationally heavy problems. This paper aims at proposing a flexible but computational efficient method for general nonparametric problems. Precisely, our proposed method is a feed-forward two-layer nonparametric estimation, and the first layer is used to learn a series of univariate basis functions for each projection variable, and then search for their optimal linear combination for each group of these learnt functions. Based on all the features derived in the first layer, the second layer attempts at learning a single index function with an unknown activation function. Our nonparametric estimation takes advantage of both random features and neural networks, and can be seen as an intermediate bridge between them.
            </div>
        </div>
    </div>
</body>
</html>

<style>

.tip {
    height: 30px;
    line-height: 30px;
}

.icon {
    width: 15px;
}

.row {
    padding: 10px; 
    height: auto; 
    border-bottom-width: 2px; 
    border-style: solid; 
    border-color: #E4E7ED; 
    padding-bottom: 20px; 
    padding-top: 20px;
    display: flex; 
    text-align: justify;
}

.left {
    min-width: 150px !important;
    text-align: center;
}

.avatar {
    width: 120px;
    height: 160px;
    max-width: 100%;
    border-radius: 10px;
}

.right {
    margin-left: 10px; 
    max-width: 80%;
}


.font-small {
    /* font-size: 16px; */
}

.font-bold {
    font-weight: bold;
}
</style>